{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hernansalinas/Curso_aprendizaje_estadistico/blob/main/Assesment/Laboratorio_redes_neuronales_convolucionales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHJO5iGJnaru"
   },
   "source": [
    "# Laboratorio redes neuronales convolucionales\n",
    "\n",
    "Objetivo: Implementar un red neuronal LeNet5 empleando keras e implementar una red neuronal  VGG.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Importar las librerias:\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from tensorflow import keras\n",
    "```\n",
    "\n",
    "2. Cargar los datos de entrenamiento y test\n",
    "```python\n",
    "(X_train,y_train),(X_test,y_test)=keras.datasets.mnist.load_data()\n",
    "```\n",
    "\n",
    "\n",
    "3. Normalizar los datos.\n",
    "\n",
    "4. Realizar una visualización de 20 imagenes aproximadamente, puede emplear el comando imshow con cmap= binary\n",
    "\n",
    "```python\n",
    "  ax.imshow(X_train[i],cmap='binary')\n",
    "```\n",
    "\n",
    "\n",
    "5. Implementar en keras, la red Letnet5, la arquitectura de la red es la siguiente:\n",
    "\n",
    "![img](https://github.com/hernansalinas/Curso_aprendizaje_estadistico/blob/main/Sesiones/convolution_img/LeNet5.png?raw=true)\n",
    "\n",
    "\n",
    "Par la implementación, se recomienda construir un modelo secuencial.\n",
    "\n",
    "\n",
    "```python\n",
    "keras.backend.clear_session()\n",
    "models=keras.models.Sequential([...])\n",
    "\n",
    "```\n",
    "\n",
    "algunos parámetros que pueden ser pasados dentro de los tres puntos anteriores son:\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.MaxPool2D,\n",
    "keras.layers.Conv2D,\n",
    "keras.layers.Flatten,\n",
    "keras.layers.Dense\n",
    "```\n",
    "\n",
    "Revisa la documentación para pasar los valores adecuados.\n",
    "\n",
    "\n",
    "6. Revisa el modelo que acabaste de construir.\n",
    "```python\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "\n",
    "7. Vamos a utlizar un optimizador de Adams,  El optimizador de Adam (Adaptive Moment Estimation) combina las ventajas de los algoritmos RMSProp y Momentum para mejorar el proceso de aprendizaje de un modelo. Al igual que Momentum, Adam utiliza una estimación del momento y de la magnitud de los gradientes  para actualizar los parámetros del modelo en cada iteración. Sin embargo, en lugar de utilizar una tasa de aprendizaje constante para todos los parámetros, Adam adapta la tasa de aprendizaje de cada parámetro individualmente en función de su estimación del momento y de la magnitud del gradiente. Esto permite que el modelo se ajuste de manera más eficiente y efectiva a los datos de entrenamiento, lo que puede llevar a una mayor precisión de la predicción en comparación con otros métodos de optimización.\n",
    "\n",
    "\n",
    "El optimizador de Adam es un algoritmo de optimización basado en gradientes de primer orden que utiliza estimaciones adaptativas de momentos de primer y segundo orden. El algoritmo se puede describir de la siguiente forma :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text{Inicializar:} \\quad \\theta_0, \\alpha, \\beta_1, \\beta_2, \\epsilon \\\\\n",
    "& \\text{Para cada iteración } t = 1, 2, \\dots \\\\\n",
    "& \\quad \\text{Obtener el gradiente:} \\quad g_t = \\nabla_\\theta J(\\theta_{t-1}) \\\\\n",
    "& \\quad \\text{Actualizar los momentos de primer orden:} \\quad m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
    "& \\quad \\text{Actualizar los momentos de segundo orden:} \\quad v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n",
    "& \\quad \\text{Corregir el sesgo de los momentos de primer orden:} \\quad \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
    "& \\quad \\text{Corregir el sesgo de los momentos de segundo orden:} \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
    "& \\quad \\text{Actualizar los parámetros:} \\quad \\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Donde $\\theta$ son los parámetros del modelo, $\\alpha$ es la tasa de aprendizaje, $\\beta_1$ y $\\beta_2$ son los factores de decaimiento de los momentos, $\\epsilon$ es un término de suavizado para evitar la división por cero, $J$ es la función objetivo, $g_t$ es el gradiente en la iteración $t$, $m_t$ y $v_t$ son los momentos de primer y segundo orden respectivamente, y $\\hat{m}_t$ y $\\hat{v}_t$ son las correcciones de sesgo de los momentos.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "emplea el siguiente compilador del modelo\n",
    "```python\n",
    " model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    " ```\n",
    "\n",
    "\n",
    " 8. Realiza el fit del modelo, emplea GPU, para ello cambia la configuración de collaboratory para que tu modelo se ejecute un poco mas rápido.\n",
    "\n",
    "\n",
    " ```python\n",
    " history = model.fit(X_train,y_train,epochs=10,validation_split=0.3)\n",
    "```\n",
    "\n",
    "\n",
    "- Loss: es la medida del error que comete el modelo al predecir las etiquetas de los datos de entrenamiento. Es una función que se quiere minimizar durante el entrenamiento. Un valor bajo de loss indica que el modelo se ajusta bien a los datos de entrenamiento.\n",
    "- Accuracy: es la medida del porcentaje de predicciones correctas que hace el modelo sobre los datos de entrenamiento. Es una métrica que se quiere maximizar durante el entrenamiento. Un valor alto de accuracy indica que el modelo clasifica bien los datos de entrenamiento.\n",
    "- Val_loss: es la medida del error que comete el modelo al predecir las etiquetas de los datos de validación. Los datos de validación son un subconjunto de los datos de entrenamiento que se reservan para evaluar el rendimiento del modelo durante el entrenamiento. Un valor bajo de val_loss indica que el modelo se generaliza bien a los datos de validación.\n",
    "- Val_accuracy: es la medida del porcentaje de predicciones correctas que hace el modelo sobre los datos de validación. Un valor alto de val_accuracy indica que el modelo clasifica bien los datos de validación.\n",
    "\n",
    "9. Realiza la predicción:\n",
    "```python\n",
    "q=model.predict(X_test)\n",
    "```\n",
    "\n",
    "10. Muestra los valores de q y determina que numero se esta prediciendo.\n",
    "\n",
    "\n",
    "11. Puede graficar la convergencia del modelo con los siguiente código\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "# Graficar la curva de loss\n",
    "plt.plot (history.history ['loss'], label='loss')\n",
    "plt.plot (history.history ['val_loss'], label='val_loss')\n",
    "plt.title ('Curva de loss')\n",
    "plt.xlabel ('Época')\n",
    "plt.ylabel ('Loss')\n",
    "plt.legend ()\n",
    "plt.show ()\n",
    "# Graficar la curva de accuracy\n",
    "plt.plot (history.history ['accuracy'], label='accuracy')\n",
    "plt.plot (history.history ['val_accuracy'], label='val_accuracy')\n",
    "plt.title ('Curva de accuracy')\n",
    "plt.xlabel ('Época')\n",
    "plt.ylabel ('Accuracy')\n",
    "plt.legend ()\n",
    "plt.show ()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "12. Una forma alterna de implementar el modelo puede ser dada de la siguiente forma:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "input_shape = (28,28,1)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(84, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "13. Emplea la arquitectura anterior para el  \n",
    "  el dataset cifar100,  empleando BatchNormalization y dropout.\n",
    "\n",
    "\n",
    "- BatchNormalization es una técnica que se usa para acelerar y estabilizar el entrenamiento de las redes neuronales artificiales, mediante la normalización de las entradas de cada capa, re-centrando y re-escalando los datos.\n",
    "\n",
    "  BatchNormalization se aplica justo antes de la función de activación de cada capa, y consiste en centrar y normalizar cada mini-lote con una media y una desviación estándar calculadas con los datos del mini-lote, y luego re-escalar y desplazar los datos de nuevo con parámetros aprendidos por la red durante el entrenamiento. Estos parámetros permiten que la red se adapte a la distribución óptima de las activaciones para cada capa. Además, al introducir cierto ruido en los datos, BatchNormalization actúa como una regularización y ayuda a reducir el sobreajuste.\n",
    "\n",
    "\n",
    "- El dropout es una técnica de regularización para reducir el sobreajuste en redes neuronales artificiales. El sobreajuste ocurre cuando el modelo se ajusta demasiado a los datos de entrenamiento y pierde capacidad de generalizar a nuevos datos. El dropout consiste en eliminar aleatoriamente algunas neuronas de la red durante el entrenamiento, lo que hace que el modelo sea más robusto y menos dependiente de ciertas conexiones. El dropout se puede interpretar como una forma de promediar varios modelos más pequeños y diferentes, lo que mejora el rendimiento final.\n",
    "\n",
    "  El dropout se aplica a cada capa de la red, especificando una probabilidad de mantener cada neurona activa. Por ejemplo, si se usa un dropout de 0.2, significa que el 20% de las neuronas se desactivarán en cada iteración del entrenamiento. El dropout solo se usa durante el entrenamiento, no durante la inferencia o la evaluación. El dropout se puede combinar con otras técnicas de regularización, como la normalización de pesos o la normalización por lotes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "(train_image, train_label) , (test_image, test_label) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "\n",
    "plt.imshow(train_image[30])\n",
    "plt.axis('off')\n",
    "```\n",
    "\n",
    "intenta agregar, lineas como las siguientes en los lugares mas estrategicos:\n",
    "\n",
    "\n",
    "```python\n",
    "keras.layers.Dropout(0.2),\n",
    "keras.layers.BatchNormalization()\n",
    "```\n",
    "\n",
    "14. Emplea el siguiente compilador:\n",
    "\n",
    "El optimizador de NAdam (Nesterov-accelerated Adaptive Moment Estimation) es una variante de Adam que incorpora el método de Nesterov, que consiste en utilizar una predicción de la posición futura de los parámetros para calcular el gradiente, en lugar de la posición actual. Esto hace que el algoritmo sea más sensible a los cambios de dirección del gradiente y evite oscilaciones innecesarias. NAdam también modifica la forma de calcular el momento y la magnitud del gradiente, usando una media móvil exponencial sesgada hacia cero en lugar de una media móvil exponencial simple.\n",
    "\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "El número de épocas que se necesita para entrenar una red neuronal depende de varios factores, como el tamaño de los datos, la complejidad del modelo, la función de pérdida, el algoritmo de optimización, la tasa de aprendizaje, etc. No hay una regla fija para elegir el número de épocas, pero se puede usar el criterio de parada temprana, que consiste en monitorear el error de validación y detener el entrenamiento cuando este empiece a aumentar, lo que indica un sobreajuste del modelo.\n",
    "\n",
    "15. Emplea early_stooping y realiza el fit\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "```\n",
    "\n",
    "El parámetro patience=5 indica el número de épocas sin mejora después de las cuales se detendrá el entrenamiento. El parámetro restore_best_weights=True indica que se restaurarán los pesos del modelo desde la época con el mejor valor de la métrica monitoreada. Esto puede ayudar a evitar el sobreajuste y mejorar el rendimiento del modelo\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "history = model.fit(train_image, train_label, epochs=30, validation_split=0.2 , batch_size=64, callbacks=[early_stopping])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDMSn6P1woYd"
   },
   "source": [
    "Ahora vamos a implementar una red VGG16, para ello puedes emplear la siguiente linea de código:\n",
    "\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "# Indian elephant\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/f/f9/Zoorashia_elephant.jpg -O indian_elephant.jpg\n",
    "# African elephant\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/b/bf/African_Elephant_%28Loxodonta_africana%29_male_%2817289351322%29.jpg -O african_elephant.jpg\n",
    "#!wget https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/african_elephant_bull.jpg?raw=true -O african_elephant.jpg\n",
    "\n",
    "# Choose the elephant to be classified\n",
    "img_path = 'african_elephant.jpg'\n",
    "\n",
    "img = mpimg.imread(img_path)\n",
    "implot = plt.imshow(img)\n",
    "\n",
    "print(\"Tamaño de la imagen:\",img.shape)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from keras.preprocessing import image\n",
    "\n",
    "model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)# Añadir una dimensión extra para el lote\n",
    "x = keras.applications.vgg16.preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)\n",
    "\n",
    "print(\"Prediction\", keras.applications.vgg16.decode_predictions(features, top=3)[0])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
