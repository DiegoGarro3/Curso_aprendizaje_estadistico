{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laboratorio03.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regresión lineal: \n",
        "\n",
        "Supongamos que tenemos un sistema en el que existe un predicor con  **m** valores de entrenamiento,  asi:   $ (x^{(1)}, y^{(1)} ) , (x^{(2)}, y^{(2)}) ... (x^{(m)}, y^{(m)})$\n",
        "\n",
        "\n",
        "|Training|Y      | X_1  |\n",
        "|--------|----------|----------|\n",
        "|1|$Y^{1}$ | $X_1^{1}$|\n",
        "|2|$Y^{2}$ | $X_1^{2}$|\n",
        "|.|.         | .        |\n",
        "|.|.         | .        |\n",
        "|.|.         | .        |\n",
        "|m|$Y^{m}$ | $X_1^{m}$  |\n",
        "\n",
        "\n",
        "\n",
        "Podemos definir un modelo lineal  como : $h(X) = \\theta_0 + \\theta_1 X$ con $(\\theta_0 , \\theta_1)$ parámetros. Nuestro objetivo es encontrar el conjunto de puntos  $(\\theta_0 , \\theta_1)$ que se encuentan más \"cercano\" a $Y$ para cada $X$.\n",
        "\n",
        "Para la optimización, vamos a definir la función de coste **$J(\\theta_1,\\theta_2 )$** para las muestras de entrenamiento como aquella que garantiza la distancia euclidiana respecto a la hipotesis planteada, así: \n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta_1, \\theta_2)=\\frac{1}{2m} \\sum_{i=0}^m ( h_{\\theta} (x^{(i)})-y^{(i)})^2\n",
        "\\end{equation}\n",
        "\n",
        "Para encontrar los valores  $(\\theta_0 , \\theta_1)$ se necesita  minimizar la función de coste, que permite obtener los valores más cercanos,  esta minimización podrá ser realizada a través de diferentes metodos el más conocido es el gradiente descendente. \n",
        "\n",
        "![](https://github.com/hernansalinas/Curso_aprendizaje_estadistico/blob/main/Sesiones/imagenes/fig00.png?raw=true=50x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tReC8I_Y8D9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supongamos un modelo lineal para realizar la predicción, así nuestro modelo estará basado en la siguiente hipotesis de trabajo:\n",
        "\n",
        "$h(X) =\\theta_0 + \\theta_1 X$\n",
        "\n",
        "\n",
        "Reemplando el modelo anterior en la función de coste, tenemos que:\n",
        "\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta_1, \\theta_2)=\\frac{1}{2m} \\sum_{i=0}^m ( (\\theta_0 + \\theta_1 X)- y^{(i)})^2\n",
        "\\end{equation}\n",
        "\n"
      ],
      "metadata": {
        "id": "Da8b8ZYU8Z4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laboratorio 03\n",
        "\n",
        "\n",
        "1. Supongamos que un conjunto de características y datos etiquetados estan dados de la siguiente manera:\n",
        " \n",
        "Crear un data frame de pandas con los siguientes datos:\n",
        "\n",
        "|Entrenamiento|Y| X_1 |\n",
        "|-|-|-|\n",
        "|0|0|0|\n",
        "|1|1|1|\n",
        "|2|2|2|\n",
        "|3|3|3|\n",
        "|4|4|4|\n",
        "|m|5|5|\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Crear una función para calcular la función de coste.\n",
        "3. Asumiendo $\\theta_0=0$, encuentre una representacion de gráfica de la función de coste para difeterentes valores de  $\\theta_1$, ¿Cuál es el mínimo de la función de coste?, con este valor, grafique sobre los datos del dataframe construido la ecuación y regresión encontrada.\n",
        "\n",
        "\n",
        "\n",
        "4. Suponiendo ahora que los datos de entrenamiento viene dados de la siguiente manera: \n",
        "\n",
        "|Entrenamiento|Y| X_1 |\n",
        "|-|-|-|\n",
        "|0|0|0|\n",
        "|1|1|1|\n",
        "|2|2|2|\n",
        "|3|3|3|\n",
        "|4|4|4|\n",
        "|m|5|5|\n",
        "\n",
        "Encontrar la función de coste para diferentes valores de $\\theta_0$, $\\theta_1$.\n",
        "Para ello puede emplar los metodos surface y contour dentro de la libreria de matplotlib.  Construya primero con los valores de $\\theta_0$, $\\theta_1 $ definidos un (np.mesgrid) y evalue  para cada punto $\\theta_0$, $\\theta_1$ la función de coste.\n",
        "\n",
        "Para el gráfico 3D puede emplear algo similar a las siguientes lineas de código\n",
        "\n",
        "```\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot_surface(t0, t1, J )\n",
        "ax.contour(t0, t1, J, 200,   linestyles=\"solid\")\n",
        "ax.set_xlabel(\"$\\\\theta_0$\")\n",
        "ax.set_ylabel(\"$\\\\theta_1$\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "En los casos anteriores la solución solo involucra 1 y 2 parámetros para la representación de la función de coste en  2D y 3D respectivamente. Cuando se tienen más parametros a optmizar, no podemos tener una representacion gráfica análoga a los casos anteriores. Notese además que los datos anteriores no tiene ningún ruido y solo estan realizados de esta manera por motivos didácticos.\n",
        "\n",
        "5. Encontrar la expresión teórica para la función de coste en el caso 1D y 2D.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wOOnW0FO9Mqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradiente Descendente\n",
        "\n",
        "\n",
        "Para determinar el minimo,  a través del gradiente descendente puede ser aplicada el siguiente algoritmo:\n",
        "- Proponer un número aleatorios inicial $\\omega_i$\n",
        "- Para descender al mínimo de la función, encontremos un  valor para el cual \n",
        "el deriva de la función descenciende, asi:\n",
        "\\begin{equation}\n",
        "\\omega_{i+1} = \\omega_{i} - \\alpha \\frac{\\mathrm{d}f(\\omega_i)}{\\mathrm{d}\\omega}\n",
        "\\end{equation}\n",
        "\n",
        "donde, $\\alpha$ es conocido como la tasa de aprendizaje del algoritmo. \n",
        "\n",
        "- Evaluar $f(\\omega_{i+1})$\n",
        "\n",
        "- Iterar hasta encontrar el minimo de la función \n",
        "\n",
        "\n",
        "6. Construya un algoritmo en el que emplee el gradiente descente para determinar el minimo de una función, determine el minimo con una error epsilon de  1E-4, pruebe su algoritmo para $f(x)= (x-4)^2$ y al menos 3 valores de $\\alpha$\n",
        "\n",
        "\n",
        "7. Para responder este punto puede consultar la siguiente  página y seguir el video[Ejemplo guia: dotcsv](https://www.youtube.com/watch?v=-_A_AAxqzCg):\n",
        "\n",
        "Encontrar el mínimo de la siguiente función a través del metodo del gradiente descendente https://en.wikipedia.org/wiki/Gradient_descent:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "F(x,y) = \\sin \\left( \\frac{1}{2}x^2-\\frac{1}{4}y^2 +3\\right) \\cos (2x+1-e^y)\n",
        "\\end{equation}\n",
        "\n",
        "  -  Para ello realice una gráfica de la función en 3D, y un mapa de contourno de la función.\n",
        "  - Determine el valor mínimo de la funcion con el metodo del gradiente descendente.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vdv89BhSENWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo de *machine learning*: Solución general\n",
        "\n",
        "Un modelo general para solucionar un problema de machining learning puede ser estructurado como sigue:\n",
        "\n",
        "\n",
        "a. Eliger el modelo a emplear:\n",
        "\\begin{equation}\n",
        "h(X,\\Theta) \n",
        "\\end{equation}\n",
        "\n",
        "- En el caso de una regresion lineal tenemos que $h(X,\\Theta=(\\theta_0, \\theta_1))$:\n",
        "\n",
        "    \\begin{equation}\n",
        "    h(X) = (\\theta_0 + \\theta_1 X)\n",
        "    \\end{equation}\n",
        "\n",
        "\n",
        "b. Eligir la funcion de coste :\n",
        "-  Metrica Ecuclidiana:\n",
        "\\begin{equation}\n",
        "J(\\Theta)=\\frac{1}{2m} \\sum_{i=1}^m ( h_{\\theta} (X^{(i)})-y^{(i)})^2\n",
        "\\end{equation}\n",
        "- [Lista de funciones de coste que pueden ser empleadas](https://jmlb.github.io/flashcards/2018/04/21/list_cost_functions_fo_neuralnets/)\n",
        "\n",
        "\n",
        "c. Aplicar el gradiente descendente iterativamente, hasta encontrar el minimo:  \n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta \\vec{\\Theta} =  - \\alpha \\nabla J(\\Theta)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "- En el caso de una regresion lineal tenemos que $h(X,\\Theta=(\\theta_0, \\theta_1))$:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_0 := \\theta_0 - \\alpha \\frac{\\partial J}{\\partial \\theta_0}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_1 := \\theta_1 - \\alpha \\frac{\\partial J}{\\partial \\theta_1}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "jQ1475YqGXXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Empleando los siguientes datos :\n",
        "\n",
        "```\n",
        "X = np.linspace(0, 1, 100)\n",
        "y = 0.2 + 0.2*X + 0.02*np.random.random(100)\n",
        "```\n",
        "\n",
        "y las herramientas desarrolladas en los apartados anteriores, \n",
        "construya un algorítmo que permita determinar una regresión lineal.\n",
        "\n",
        "9. Compare su resultado empleando la libreria linearRegresion() de sklearn.\n"
      ],
      "metadata": {
        "id": "lJs2ytT8IVW4"
      }
    }
  ]
}
